import{_ as e,o as t,c as l,a6 as a}from"./chunks/framework.BB0md0jN.js";const k=JSON.parse('{"title":"自然语言处理：基于预训练模型的方法","description":"","frontmatter":{"layout":"doc","title":"自然语言处理：基于预训练模型的方法","readingTime":"15 min read"},"headers":[],"relativePath":"电影和阅读/微信读书/自然语言处理_基于预训练模型的方法.md","filePath":"电影和阅读/微信读书/自然语言处理_基于预训练模型的方法.md"}'),c={name:"电影和阅读/微信读书/自然语言处理_基于预训练模型的方法.md"};function r(u,o,p,b,q,n){return t(),l("div",null,[...o[0]||(o[0]=[a('<h1 id="自然语言处理-基于预训练模型的方法" tabindex="-1">自然语言处理：基于预训练模型的方法 <a class="header-anchor" href="#自然语言处理-基于预训练模型的方法" aria-label="Permalink to &quot;自然语言处理：基于预训练模型的方法&quot;">​</a></h1><p><img src="https://wfqqreader-1252317822.image.myqcloud.com/cover/53/40795053/t6_40795053.jpg" alt=" 自然语言处理：基于预训练模型的方法"></p><ul><li><strong>书名</strong>： 自然语言处理：基于预训练模型的方法</li><li><strong>作者</strong>： 车万翔 郭江 崔一鸣</li><li><strong>简介</strong>： 自然语言处理被誉为“人工智能皇冠上的明珠”。深度学习等技术的引入为自然语言处理技术带来了一场革命，尤其是近年来出现的基于预训练模型的方法，已成为研究自然语言处理的新范式。本书在介绍自然语言处理、深度学习等基本概念的基础上，重点介绍新的基于预训练模型的自然语言处理技术。本书包括基础知识、预训练词向量和预训练模型三大部分：基础知识部分介绍自然语言处理和深度学习的基础知识和基本工具；预训练词向量部分介绍静态词向量和动态词向量的预训练方法及应用；预训练模型部分介绍几种典型的预训练语言模型及应用，以及预训练模型的最新进展。除了理论知识，本书还有针对性地结合具体案例提供相应的PyTorch代码实现，不仅能让读者对理论有更深刻的理解，还能快速地实现自然语言处理模型，达到理论和实践的统一。本书既适合具有一定机器学习基础的高等院校学生、研究机构的研究者，以及希望深入研究自然语言处理算法的计算机工程师阅读，也适合对人工智能、深度学习和自然语言处理感兴趣的学生和希望进入人工智能应用领域的研究者参考。</li><li><strong>出版时间</strong>： 2021-07-01 00:00:00</li><li><strong>ISBN</strong>： 9787121415128</li><li><strong>分类</strong>： 计算机-人工智能</li><li><strong>出版社</strong>： 电子工业出版社</li><li><strong>PC地址</strong>： <a href="https://weread.qq.com/web/reader/03232b70726e7bad032cc89" target="_blank" rel="noreferrer">https://weread.qq.com/web/reader/03232b70726e7bad032cc89</a></li></ul><h2 id="第1章-绪论" tabindex="-1">第1章 绪论 <a class="header-anchor" href="#第1章-绪论" aria-label="Permalink to &quot;第1章 绪论&quot;">​</a></h2><blockquote><p>📌 ，经验主义又被分成了基于统计模型、深度学习模型及最新的预训练模型三个阶段，尤其是“预训练+精调”的方式，已成为自然语言处理的最新范式。 ⏱ 2023-11-19 12:05:51</p></blockquote><h3 id="_1-4-自然语言处理技术发展历史" tabindex="-1">1.4 自然语言处理技术发展历史 <a class="header-anchor" href="#_1-4-自然语言处理技术发展历史" aria-label="Permalink to &quot;1.4 自然语言处理技术发展历史&quot;">​</a></h3><blockquote><p>📌 需要事先利用经验性规则将原始的自然语言输入转化为机器能够处理的向量形式。这一转化过程（也称为特征提取）需要细致的人工操作和一定的专业知识，因此也被称为特征工程。 ⏱ 2023-11-19 11:40:31</p></blockquote><blockquote><p>📌 所谓表示学习，是指机器能根据输入自动地发现可以用于识别或分类等任务的表示。具体地，深度学习模型在结构上通常包含多层的处理层。底层的处理层接收原始输入，然后对其进行抽象处理，其后的每一层都在前一层的结果上进行更深层次的抽象，最后一层的抽象结果即为输入的一个表示，用于最终的目标任务。其中的抽象处理，是由模型内部的参数进行控制的，而参数的更新值则是根据训练数据上模型的表现，使用反向传播算法学习得到的。 ⏱ 2023-11-19 11:42:38</p></blockquote><blockquote><p>📌 深度学习可以有效地避免统计学习方法中的人工特征提取操作，自动地发现对于目标任务有效的表示。 ⏱ 2023-11-19 11:42:44</p></blockquote><blockquote><p>📌 而表示学习能够将不同任务在相同的向量空间内进行表示，从而具备跨任务迁移的能力。除了可以跨任务，还可以实现跨语言甚至跨模态的迁移。综合利用多项任务、多种语言和多个模态的数据，使得人工智能向更通用的方向迈进了一步。 ⏱ 2023-11-19 11:43:26</p></blockquote><blockquote><p>📌 随着基于深度学习的序列到序列生成框架的提出，这种逐词的文本生成方法全面提升了生成技术的灵活性和实用性，完全革新了机器翻译、文本摘要和人机对话等任务的技术范式。 ⏱ 2023-11-19 11:46:33</p></blockquote><blockquote><p>📌 但是基于深度学习的算法有一个致命的缺点，就是过度依赖于大规模有标注数据。对于语音识别、图像处理等感知类任务，标注数据相对容易获得，如：在图像处理领域，人们已经为上百万幅的图像标注了相应的类别（如ImageNet数据集）；用于语音识别的“语音--文本”平行语料库也有几十万小时。 ⏱ 2023-11-19 11:51:40</p></blockquote><blockquote><p>📌 所面对的任务和领域众多，使得标注大规模语料库的时间过长，人力成本过于高昂，因此自然语言处理的标注数据往往不够充足，很难满足深度学习模型训练的需要。 ⏱ 2023-11-19 11:52:00</p></blockquote><blockquote><p>📌 其实，文本自身的顺序性就是一种天然的标注数据，通过若干连续出现的词语预测下一个词语（又称语言模型）就可以构成一项原任务。由于图书、网页等文本数据规模近乎无限，所以，可以非常容易地获得超大规模的预训练数据。有人将这种不需要人工标注数据的预训练学习方法称为无监督学习（Unsupervised Learning）， ⏱ 2023-11-19 11:57:28</p></blockquote><blockquote><p>📌 为了能够刻画大规模数据中复杂的语言现象，还要求所使用的深度学习模型容量足够大。基于自注意力的Transformer模型显著地提升了对于自然语言的建模能力，是近年来具有里程碑意义的进展之一。要想在可容忍的时间内，在如此大规模的数据上训练一个超大规模的Transformer模型，也离不开以GPU、TPU为代表的现代并行计算硬件。可以说，超大规模预训练语言模型完全依赖“蛮力”，在大数据、大模型和大算力的加持下，使自然语言处理取得了长足的进步。 ⏱ 2023-11-19 12:01:04</p></blockquote><h2 id="第2章-自然语言处理基础" tabindex="-1">第2章 自然语言处理基础 <a class="header-anchor" href="#第2章-自然语言处理基础" aria-label="Permalink to &quot;第2章 自然语言处理基础&quot;">​</a></h2><blockquote><p>📌 本章首先介绍自然语言处理中最基础、最本质的问题，即文本如何在计算机内表示，才能达到易于处理和计算的目的。其中，词的表示大体经过了早期的独热（One-hot）表示，到后来的分布式表示，再到最近的词向量三个阶段。 ⏱ 2023-11-19 11:32:17</p></blockquote><blockquote><p>📌 基础任务包括中文分词、词性标注、句法分析和语义分析等，应用任务包括信息抽取、情感分析、问答系统、机器翻译和对话系统等 ⏱ 2023-11-19 11:37:37</p></blockquote><blockquote><p>📌 这些任务基本可以归纳为文本分类、结构预测和序列到序列三大类问题，所以同时介绍这三大类问题的解决思路 ⏱ 2023-11-19 11:37:55</p></blockquote><h3 id="_2-1-文本的表示" tabindex="-1">2.1 文本的表示 <a class="header-anchor" href="#_2-1-文本的表示" aria-label="Permalink to &quot;2.1 文本的表示&quot;">​</a></h3><blockquote><p>📌 这种基于规则的方法存在很多问题。首先，规则的归纳依赖专家的经验，需要花费大量的人力、物力和财力；其次，规则的表达能力有限，很多语言现象无法用简单的规则描述；最后，随着规则的增多，规则之间可能存在矛盾和冲突的情况，导致最终无法做出决策。 ⏱ 2023-11-19 12:14:09</p></blockquote><blockquote><p>📌 基于机器学习的自然语言处理技术应运而生，其最本质的思想是将文本表示为向量，其中的每一维代表一个特征。 ⏱ 2023-11-19 12:14:25</p></blockquote><blockquote><p>📌 文本向量表示还可以用于计算两个文本之间的相似度，即使用余弦函数等度量函数表示两个向量之间的相似度， ⏱ 2023-11-19 12:16:20</p></blockquote><blockquote><p>📌 在该向量中，词表中第i个词在第i维上被设置为1，其余维均为0。这种表示被称为词的独热表示或独热编码（One-hot Encoding）。独热表示的一个主要问题就是不同词使用完全不同的向量进行表示，这会导致即使两个词在语义上很相似，但是通过余弦函数来度量它们之间的相似度时值却为0。 ⏱ 2023-11-19 12:23:32</p></blockquote><blockquote><p>📌 为了缓解数据稀疏问题，传统的做法是除了词自身，再提取更多和词相关的泛化特征，如词性特征、词义特征和词聚类特征等。以语义特征为例，通过引入WordNet[1]等语义词典，可以获知“漂亮”和“美丽”是同义词，然后引入它们的共同语义信息作为新的额外特征，从而缓解同义词的独热表示不同的问题。可以说，在使用传统机器学习方法解决自然语言处理问题时，研究者的很大一部分精力都用在了挖掘有效的特征上。 ⏱ 2023-11-19 12:25:50</p></blockquote><blockquote><p>📌 词的独热表示容易导致数据稀疏问题，而通过引入特征的方法虽然可以缓解该问题，但是特征的设计费时费力。那么有没有办法自动提取特征并设置相应的特征值呢？ ⏱ 2023-11-19 13:02:13</p></blockquote><blockquote><p>📌 分布式语义假设仅仅提供了一种语义建模的思想。具体到表示形式和上下文的选择，以及如何利用上下文的分布特征，都是需要解决的问题。 ⏱ 2023-11-19 13:02:41</p></blockquote><blockquote><p>📌 ，由于有共同的上下文“我”和“学习”，使得它们之间具有了一定的相似性，而不是如独热表示一样，没有任何关系。 ⏱ 2023-11-19 13:03:47</p></blockquote><blockquote><p>📌 仍然存在稀疏性的问题。即向量中仍有大量的值为0，这一点从表2-1中也可以看出。下面分别介绍如何通过点互信息和奇异值分解两种技术来解决这些问题。 ⏱ 2023-11-19 13:04:45</p></blockquote><blockquote><p>📌 信息论中的点互信息（Pointwise Mutual Information，PMI）恰好能够做到这一点。对于词w和上下文c，其PMI为：[插图]式中，P （w， c）、P （w）、P （c）分别是w与c的共现概率，以及w和c分别出现的概率。可见，通过PMI公式计算，如果w和c的共现概率（与频次正相关）较高，但是w或者c出现的概率也较高（高频词），则最终的PMI值会变小；反之，即便w和c的共现概率不高，但是w或者c出现的概率较低（低频词），则最终的PMI值也可能会比较大。从而较好地解决高频词误导计算结果的问题。 ⏱ 2023-11-19 13:05:28</p></blockquote><h3 id="_3-3-pytorch基础" tabindex="-1">3.3 PyTorch基础 <a class="header-anchor" href="#_3-3-pytorch基础" aria-label="Permalink to &quot;3.3 PyTorch基础&quot;">​</a></h3><blockquote><p>📌 基本的数据存储结构——张量 ⏱ 2023-03-03 01:34:08</p></blockquote>',32)])])}const s=e(c,[["render",r]]);export{k as __pageData,s as default};
