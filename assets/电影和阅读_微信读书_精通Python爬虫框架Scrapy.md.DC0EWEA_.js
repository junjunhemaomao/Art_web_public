import{_ as t,o as a,c as e,a6 as o}from"./chunks/framework.BB0md0jN.js";const d=JSON.parse('{"title":"精通Python爬虫框架Scrapy","description":"","frontmatter":{"layout":"doc","title":"精通Python爬虫框架Scrapy","readingTime":"4 min read"},"headers":[],"relativePath":"电影和阅读/微信读书/精通Python爬虫框架Scrapy.md","filePath":"电影和阅读/微信读书/精通Python爬虫框架Scrapy.md"}'),c={name:"电影和阅读/微信读书/精通Python爬虫框架Scrapy.md"};function p(s,r,n,i,l,y){return a(),e("div",null,[...r[0]||(r[0]=[o('<h1 id="精通python爬虫框架scrapy" tabindex="-1">精通Python爬虫框架Scrapy <a class="header-anchor" href="#精通python爬虫框架scrapy" aria-label="Permalink to &quot;精通Python爬虫框架Scrapy&quot;">​</a></h1><p><img src="https://wfqqreader-1252317822.image.myqcloud.com/cover/206/22692206/t6_22692206.jpg" alt=" 精通Python爬虫框架Scrapy"></p><ul><li><strong>书名</strong>： 精通Python爬虫框架Scrapy</li><li><strong>作者</strong>： 迪米特里奥斯 考奇斯-劳卡斯</li><li><strong>简介</strong>： 《精通Python爬虫框架Scrapy》以Scrapy 1.0版本为基础，讲解了Scrapy的基础知识，以及如何使用Python和三方API提取、整理数据，以满足自己的需求。 本书共11章，其内容涵盖了Scrapy基础知识，理解HTML和XPath，安装Scrapy并爬取一个网站，使用爬虫填充数据库并输出到移动应用中，爬虫的强大功能，将爬虫部署到Scrapinghub云服务器，Scrapy的配置与管理，Scrapy编程，管道秘诀，理解Scrapy性能，使用Scrapyd与实时分析进行分布式爬取。本书附录还提供了各种软件的安装与故障排除等内容。</li><li><strong>出版时间</strong>： 2018-02-01 00:00:00</li><li><strong>ISBN</strong>： 9787115474209</li><li><strong>分类</strong>： 计算机-编程设计</li><li><strong>出版社</strong>： 人民邮电出版社</li><li><strong>PC地址</strong>： <a href="https://weread.qq.com/web/reader/6cf32bc0715a416e6cf7781" target="_blank" rel="noreferrer">https://weread.qq.com/web/reader/6cf32bc0715a416e6cf7781</a></li></ul><h3 id="_1-1-初识scrapy" tabindex="-1">1.1 初识Scrapy <a class="header-anchor" href="#_1-1-初识scrapy" aria-label="Permalink to &quot;1.1 初识Scrapy&quot;">​</a></h3><blockquote><p>📌 可以并行运行多个请求，并通过单一线程来管理它们。这意味着更低的主机托管费用，与其他应用的协作机会，以及相比于传统多线程应用而言更简单的代码 ⏱ 2020-01-28 14:28:16</p></blockquote><h3 id="_2-2-使用xpath选择html元素" tabindex="-1">2.2 使用XPath选择HTML元素 <a class="header-anchor" href="#_2-2-使用xpath选择html元素" aria-label="Permalink to &quot;2.2 使用XPath选择HTML元素&quot;">​</a></h3><blockquote><p>📌 //p将会选择所有的p元素，而//a则会选择所有的链接。 ⏱ 2020-02-06 05:02:04</p></blockquote><blockquote><p>📌 $x(&#39;//a&#39;) [ <a href="http://www.iana.org/domains/example">More information...</a> ] ⏱ 2020-02-06 05:05:43</p></blockquote><blockquote><p>📌 要想找到div元素下的所有链接，可以使用//div//a。 ⏱ 2020-02-06 05:06:49</p></blockquote><h3 id="_3-3-一个scrapy项目" tabindex="-1">3.3 一个Scrapy项目 <a class="header-anchor" href="#_3-3-一个scrapy项目" aria-label="Permalink to &quot;3.3 一个Scrapy项目&quot;">​</a></h3><blockquote><p>📌 到目前为止，我们只是在通过scrapy shell“小打小闹”。现在，既然已经拥有了用于开始第一个Scrapy项目的所有必要组成部分，那么让我们按下Ctrl+D退出scrapy shell吧。需要注意的是，你现在输入的所有内容都将丢失。显然，我们并不希望在每次爬取某些东西的时候都要输入代码，因此一定要谨记scrapy shell只是一个可以帮助我们调试页面、XPath表达式和Scrapy对象的工具。不要花费大量时间在这里编写复杂代码，因为一旦你退出，这些代码就都会丢失。为了编写真实的Scrapy代码，我们将使用项目。下面创建一个Scrapy项目，并将其命名为&quot;properties&quot;，因为我们正在抓取的数据是房产。 $ scrapy startproject properties $ cd properties $ tree . ├── properties | ├── <strong>init</strong>.py | ├── items.py | ├── pipelines.py | ├── settings.py | └── spiders | └── <strong>init</strong>.py └── scrapy.cfg 2 directories, 6 files [插图] 提醒一下，你可以从GitHub中获得本书的全部源代码。要下载该代码，可以使用如下命令： git clone <a href="https://github.com/scalingexcellence/" target="_blank" rel="noreferrer">https://github.com/scalingexcellence/</a> scrapybook 本章的代码在ch03目录中，其中该示例的代码在ch03/properties目录中。 我们可以看到这个Scrapy项目的目录结构。命令scrapy startproject properties创建了一个以项目名命名的目录，其中包含3个我们感兴趣的文件，分别是items. ⏱ 2020-12-12 09:51:01</p></blockquote>',11)])])}const g=t(c,[["render",p]]);export{d as __pageData,g as default};
