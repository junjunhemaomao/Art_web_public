import{_ as e,o as t,c as a,a6 as l}from"./chunks/framework.BB0md0jN.js";const p=JSON.parse('{"title":"Python自然语言处理实战：核心技术与算法","description":"","frontmatter":{"layout":"doc","title":"Python自然语言处理实战：核心技术与算法","readingTime":"7 min read"},"headers":[],"relativePath":"电影和阅读/微信读书/计算机_编程设计/Python自然语言处理实战_核心技术与算法.md","filePath":"电影和阅读/微信读书/计算机_编程设计/Python自然语言处理实战_核心技术与算法.md"}'),r={name:"电影和阅读/微信读书/计算机_编程设计/Python自然语言处理实战_核心技术与算法.md"};function c(i,o,b,n,u,q){return t(),a("div",null,[...o[0]||(o[0]=[l('<h1 id="python自然语言处理实战-核心技术与算法" tabindex="-1">Python自然语言处理实战：核心技术与算法 <a class="header-anchor" href="#python自然语言处理实战-核心技术与算法" aria-label="Permalink to &quot;Python自然语言处理实战：核心技术与算法&quot;">​</a></h1><p><img src="https://wfqqreader-1252317822.image.myqcloud.com/cover/151/24953151/t6_24953151.jpg" alt=" Python自然语言处理实战：核心技术与算法"></p><ul><li><strong>书名</strong>： Python自然语言处理实战：核心技术与算法</li><li><strong>作者</strong>： 涂铭 刘祥 刘树春</li><li><strong>简介</strong>： 这是一本关于中文自然语言处理（简称NLP）的书，NLP是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。NLP是一门融语言学、计算机科学、数学于一体的科学。本书偏重实战，不仅系统介绍了NLP涉及的知识点，同时也教会读者如何实际应用与开发。</li><li><strong>出版时间</strong>： 2018-04-01 00:00:00</li><li><strong>ISBN</strong>： 9787111597674</li><li><strong>分类</strong>： 计算机-编程设计</li><li><strong>出版社</strong>： 机械工业出版社</li><li><strong>PC地址</strong>： <a href="https://weread.qq.com/web/reader/0b132940717cc13f0b1bb05" target="_blank" rel="noreferrer">https://weread.qq.com/web/reader/0b132940717cc13f0b1bb05</a></li></ul><h2 id="第3章-中文分词技术" tabindex="-1">第3章 中文分词技术 <a class="header-anchor" href="#第3章-中文分词技术" aria-label="Permalink to &quot;第3章 中文分词技术&quot;">​</a></h2><blockquote><p>📌 在语言理解中，词是最小的能够独立活动的有意义的语言成分。 ⏱ 2020-12-14 18:58:30</p></blockquote><h3 id="_3-1-中文分词简介" tabindex="-1">3.1 中文分词简介 <a class="header-anchor" href="#_3-1-中文分词简介" aria-label="Permalink to &quot;3.1 中文分词简介&quot;">​</a></h3><blockquote><p>📌 而在汉语中，词以字为基本单位的，但是一篇文章的语义表达却仍然是以词来划分的。 ⏱ 2020-12-14 19:07:06</p></blockquote><blockquote><p>📌 主要归纳为“规则分词”“统计分词”和“混合分词（规则+统计）”这三个主要流派。 ⏱ 2020-12-14 19:08:17</p></blockquote><blockquote><p>📌 实践中多是采用这两种方法的结合，即混合分词。 ⏱ 2020-12-14 19:08:48</p></blockquote><h3 id="_3-2-规则分词" tabindex="-1">3.2 规则分词 <a class="header-anchor" href="#_3-2-规则分词" aria-label="Permalink to &quot;3.2 规则分词&quot;">​</a></h3><blockquote><p>📌 按照匹配切分的方式，主要有正向最大匹配法、逆向最大匹配法以及双向最大匹配法三种方法。 ⏱ 2020-12-14 19:09:38</p></blockquote><blockquote><p>📌 这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。</p></blockquote><ul><li>💭 遍历数据库匹配样本 - ⏱ 2020-12-14 19:10:32</li></ul><blockquote><p>📌 逆向最大匹配法从被处理文档的末端开始匹配扫描， ⏱ 2020-12-14 19:28:08</p></blockquote><blockquote><p>📌 相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。 ⏱ 2020-12-14 19:28:44</p></blockquote><blockquote><p>📌 由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。所以，逆向最大匹配法比正向最大匹配法的误差要小。 ⏱ 2020-12-14 19:29:54</p></blockquote><blockquote><p>📌 双向最大匹配法（Bi-directction Matching method）是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。 ⏱ 2020-12-14 19:35:09</p></blockquote><blockquote><p>📌 中文中90.0%左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0%的句子两种切分方法得到的结果不一样 ⏱ 2020-12-14 20:03:41</p></blockquote><h3 id="_3-3-统计分词" tabindex="-1">3.3 统计分词 <a class="header-anchor" href="#_3-3-统计分词" aria-label="Permalink to &quot;3.3 统计分词&quot;">​</a></h3><blockquote><p>📌 基于统计的中文分词算法渐渐成为主流。其主要思想是把每个词看做是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现的次数越多，就证明这相连的字很可能就是一个词。 ⏱ 2020-12-14 20:51:52</p></blockquote><blockquote><p>📌 当组合频度高于某一个临界值时，我们便可认为此字组可能会构成一个词语。 ⏱ 2020-12-14 20:52:07</p></blockquote><blockquote><p>📌 对比机械分词法，这些统计分词方法不需耗费人力维护词典，能较好地处理歧义和未登录词，是目前分词中非常主流的方法。但其分词的效果很依赖训练语料的质量，且计算量相较于机械分词要大得多。 ⏱ 2020-12-14 20:57:49</p></blockquote><h3 id="_3-4-混合分词" tabindex="-1">3.4 混合分词 <a class="header-anchor" href="#_3-4-混合分词" aria-label="Permalink to &quot;3.4 混合分词&quot;">​</a></h3><blockquote><p>📌 事实上，目前不管是基于规则的算法、还是基于HMM、CRF或者deep learning等的方法，其分词效果在具体任务中，其实差距并没有那么明显。 ⏱ 2020-12-14 20:58:28</p></blockquote><blockquote><p>📌 最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。 ⏱ 2020-12-14 20:58:42</p></blockquote><blockquote><p>📌 下节介绍的Jieba分词工具便是基于这种方法的实现。 ⏱ 2020-12-14 21:02:28</p></blockquote><h3 id="_3-5-中文分词工具——jieba" tabindex="-1">3.5 中文分词工具——Jieba <a class="header-anchor" href="#_3-5-中文分词工具——jieba" aria-label="Permalink to &quot;3.5 中文分词工具——Jieba&quot;">​</a></h3><blockquote><p>📌 Jieba分词结合了基于规则和基于统计这两类方法。 ⏱ 2020-12-14 21:06:09</p></blockquote><blockquote><p>📌 ，基于前缀词典可以快速构建包含全部可能分词结果的有向无环图，这个图中包含多条分词路径，有向是指全部的路径都始于第一个字、止于最后一个字，无环是指节点之间不构成闭环。基于标注语料，使用动态规划的方法可以找出最大概率路径，并将其作为最终的分词结果。 ⏱ 2020-12-14 21:09:24</p></blockquote><blockquote><p>📌 一般直接使用精确模式即可， ⏱ 2020-12-14 21:11:35</p></blockquote><blockquote><p>📌 高频词提取其实就是自然语言处理中的TF（Term Frequency）策略 ⏱ 2020-12-14 21:12:03</p></blockquote><h2 id="_3-2-规则分词-1" tabindex="-1">3.2 规则分词 <a class="header-anchor" href="#_3-2-规则分词-1" aria-label="Permalink to &quot;3.2 规则分词&quot;">​</a></h2><h3 id="划线评论" tabindex="-1">划线评论 <a class="header-anchor" href="#划线评论" aria-label="Permalink to &quot;划线评论&quot;">​</a></h3><blockquote><p>📌 这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。<br> - 💭 遍历数据库匹配样本 - ⏱ 2020-12-14 19:11:12</p></blockquote>',34)])])}const _=e(r,[["render",c]]);export{p as __pageData,_ as default};
