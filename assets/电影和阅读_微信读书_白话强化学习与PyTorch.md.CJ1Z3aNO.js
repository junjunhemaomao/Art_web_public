import{_ as e,o as t,c as l,a6 as c}from"./chunks/framework.BB0md0jN.js";const n=JSON.parse('{"title":"白话强化学习与PyTorch","description":"","frontmatter":{"layout":"doc","title":"白话强化学习与PyTorch","readingTime":"17 min read"},"headers":[],"relativePath":"电影和阅读/微信读书/白话强化学习与PyTorch.md","filePath":"电影和阅读/微信读书/白话强化学习与PyTorch.md"}'),u={name:"电影和阅读/微信读书/白话强化学习与PyTorch.md"};function r(a,o,p,b,q,k){return t(),l("div",null,[...o[0]||(o[0]=[c('<h1 id="白话强化学习与pytorch" tabindex="-1">白话强化学习与PyTorch <a class="header-anchor" href="#白话强化学习与pytorch" aria-label="Permalink to &quot;白话强化学习与PyTorch&quot;">​</a></h1><p><img src="https://cdn.weread.qq.com/weread/cover/29/YueWen_27741172/t6_YueWen_27741172.jpg" alt=" 白话强化学习与PyTorch"></p><ul><li><strong>书名</strong>： 白话强化学习与PyTorch</li><li><strong>作者</strong>： 高扬 叶振斌</li><li><strong>简介</strong>： 本书以“平民”的起点，从“零”开始，基于PyTorch框架，介绍深度学习和强化学习的技术与技巧，逐层铺垫，营造良好的带入感和亲近感，把学习曲线拉平，使得没有学过微积分等高级理论的程序员一样能够读得懂、学得会。同时，本书配合漫画插图来调节阅读气氛，并对每个原理都进行了对比讲解和实例说明。</li><li><strong>出版时间</strong>： 2019-09-01 00:00:00</li><li><strong>ISBN</strong>： 9787121367472</li><li><strong>分类</strong>： 计算机-理论知识</li><li><strong>出版社</strong>： 电子工业出版社</li><li><strong>PC地址</strong>： <a href="https://weread.qq.com/web/reader/e7d327c071a74bf4e7d0dfb" target="_blank" rel="noreferrer">https://weread.qq.com/web/reader/e7d327c071a74bf4e7d0dfb</a></li></ul><h3 id="第1章-强化学习是什么" tabindex="-1">第1章 强化学习是什么 <a class="header-anchor" href="#第1章-强化学习是什么" aria-label="Permalink to &quot;第1章 强化学习是什么&quot;">​</a></h3><blockquote><p>📌 。监督学习是一个比较传统的机器学习研究领域。简单概括，监督学习主要希望研究映射关系y=f(x|θ). ⏱ 2023-10-04 20:52:07</p></blockquote><blockquote><p>📌 这里的θ就是x前面的系数2和后面的常数项3。</p></blockquote><ul><li>💭 特征值 - ⏱ 2023-10-08 08:46:41</li></ul><blockquote><p>📌 如果我们实际观测到的值只有x=2、y=7，但不知道θ的值，希望通过一系列科学的办法“反向”推导出θ的值，那么，这就属于机器学习的研究范畴了 ⏱ 2023-10-08 08:47:12</p></blockquote><blockquote><p>📌 在有足够的x和对应的y的值的时候，尝试用线性回归的方法求出θ的值。将已知输入变量x和输出变量y的值作为参数，逐步求出待定系数θ的过程，就是训练（Training）过程。当通过这样一个过程学习到θ应该为什么值之后，如果再有x出现，就可以通过函数y=f(x|θ)计算出y的值。这就是监督学习的实际工作方式，这个例子就是典型的监督学习中的线性回归（Linear Regression）问题。 ⏱ 2024-11-13 21:13:33</p></blockquote><blockquote><p>📌 ，这个过程与研究y=f(x|θ)的过程（需要知道x和y的具体值，才能完成整个训练过程）不一样。在这个例子中，我们只需要知道x是什么就够了——x就是空间中的一个个点。 ⏱ 2023-10-08 08:49:55</p></blockquote><blockquote><p>📌 还有很多机器学习的研究领域，例如迁移学习（Transfer Learning）、生成对抗网络（GenerativeAdversarial Networks, GAN）等以不同建模方式或思考角度来解决问题的方法论。 ⏱ 2024-11-13 20:31:31</p></blockquote><blockquote><p>📌 谈不上理解并复现人类智慧进化的精华。要知道，在生命科学层面的认知上，人类还有很长的路要走。人类对自己的认知和了解的过程，其难度就像自举——自己抓着自己的头把自己举起来——岂止困难，简直不可能。 ⏱ 2023-10-08 08:52:02</p></blockquote><blockquote><p>📌 我们还能拍着胸脯说自己理解了智慧生物的承载本质吗？从这个角度说这些技术是“形似”，都是客气的。 ⏱ 2024-11-13 21:14:59</p></blockquote><blockquote><p>📌 ，其中的一个项目组为了让一个神经网络能够认出视频中的物体，找了一群大学生来做打标签的工作（标出图片中的物体是什么东西、在什么位置），然后把数以万计打了标签的图片放到网络中去训练，从而让网络正确识别这些被标记物体的位置和分类。在这个过程中，超过60% 的人工数（人×天）花在了标记样本上。当时，业界还有几个从事相关领域研究的大型机构，在这些机构的工作人员中间流传着这样一句话：有多少“人工”，就有多少“智能”。 ⏱ 2023-10-08 08:53:33</p></blockquote><blockquote><p>📌 这种人工智能仅仅是一个高质量的自动化过程，只要它能在人类关心的范畴高质量地完成作业就够了。 ⏱ 2023-10-08 08:58:33</p></blockquote><blockquote><p>📌 只要它能够在应用范畴提高自动化程度、降低错误率，或者能在错误率与人类相当的情况下极大地提高处理效率，给出的结果和我们期望中由一个人来完成工作的结果很接近，就可以了。 ⏱ 2023-10-08 08:58:47</p></blockquote><blockquote><p>📌 强化学习的主要目的是研究并解决机器人智能体贯序决策问题。 ⏱ 2023-10-08 09:02:51</p></blockquote><blockquote><p>📌 在这样一个完整的题设下，机器人应该尽可能在没有人干预的情况下，不断根据周围的环境变化学会并判断“在什么情况下怎么做才最好”，从而一步一步完成一个完整的任务。这样一系列针对不同情形的最合理的行为组合逻辑，才是一个完整的策略，而非一个简单而孤立的行为。 ⏱ 2023-10-08 09:03:35</p></blockquote><blockquote><p>📌 通过一系列科学的方法，对这类普适性问题进行体系性的求解方式和方法的归纳，才是这么多强化学习算法要解决的核心问题。 ⏱ 2023-10-08 09:11:08</p></blockquote><blockquote><p>📌 。如果不知道整棵树的层级、分叉和每个树杈的权重，是没有办法用直接遍历的方法得到一个完整路径的行进策略的。 ⏱ 2023-10-08 09:11:33</p></blockquote><blockquote><p>📌 千万别忘了计算机是干什么用的。它能思考吗？不能。它能做的事情太单纯了，只能做加、减、乘、除运算，而且乘、除、减都是由加模拟出来的。除此之外，就是比大小。再多的处理过程，也都是加、减、乘、除和比大小这样的简单操作的组合——没办法，这是计算机天生的能力（或者说局限性）所带来的 ⏱ 2023-10-08 09:21:54</p></blockquote><blockquote><p>📌 损失函数Loss=∑|f(xi|θ)−yi|，通过最优化问题（Optimization Problem）的思路或方法来求解当损失函数Loss(θ) 取极小值时θ的值。这就是绝大多数监督学习方法的整体思路，f(x|θ)就是我们想要得到的模型表达式。 ⏱ 2023-10-08 20:09:18</p></blockquote><blockquote><p>📌 当然，一个机器学习问题也有可能不被转换成求解一个函数的极小值的问题，而被转换成求解其极大值的问题。例如，待优化的函数在描述得分、匹配度、适应度等积极评价标准的时候，就是在求极大值了。 ⏱ 2023-10-08 20:13:11</p></blockquote><blockquote><p>📌 但问题是，我们并非每次都有“特权”，确切地说，我们在绝大多数情况下都不可能有“特权”。 ⏱ 2023-10-08 20:14:03</p></blockquote><blockquote><p>📌 从悲观的角度思考，强化学习成了一种无奈的选择，是在无法通过直接开启“上帝视角”解决问题的情况下采取的仿照人类探索自然环境的方法去建模的手段。 ⏱ 2023-10-08 20:14:47</p></blockquote><blockquote><p>📌 现在的麻烦是，如果这棵树的形态不确定，那么我们不仅没有办法用普通的树遍历方法来解决问题，而且必须先构造一个用于尝试的机器人来构建完整的树（这个机器人会在这个陌生的世界里通过试探的方式来构建一棵树）。 ⏱ 2023-10-08 20:16:56</p></blockquote><blockquote><p>📌 。因为我们无法开启“上帝视角”，所以只能在非常有限的视野里观测周围的情况——是的，非常有限。这种建模方式本身也是由人类自身的认知局限性产生的。无论是人，还是机器人，能够感知的范围和内容都是有限的 ⏱ 2023-10-09 08:40:25</p></blockquote><blockquote><p>📌 什么是超参数呢？就是无法通过训练自动学会的参数。 ⏱ 2023-10-09 08:42:30</p></blockquote><blockquote><p>📌 超参数则必须在整个训练开始之前确定，因为在训练过程中没有办法通过学习来确定这个值。 ⏱ 2023-10-09 08:42:26</p></blockquote><blockquote><p>📌 器人在一个环境中不断地试探，就会不断地得到这样一个短序列：St→At→Rt→St+1. ⏱ 2023-10-09 08:43:20</p></blockquote><blockquote><p>📌 其实，将短序列St→At→Rt→St+1作为观测对象来建模也实属无奈。因为这样做无法得到一棵完整的树，所以才不得不用边做动作边记录的方式来收集相应的数据。在一些资料中，会将这些短序列称为S→S′Transition，这就是在强化学习的建模环节所能观测到的有意义的最小单位 ⏱ 2023-10-09 08:44:40</p></blockquote><blockquote><p>📌 只能通过反复实践，把这些客观观测到的现象所对应的人类行为及后果片段性地记录下来——连续记录。当积累一定量的数据之后，后世的人就能从中品味、总结，得到一些智慧的启迪了。 ⏱ 2023-10-09 08:44:55</p></blockquote><blockquote><p>📌 ，下标t和t+1指的是某一时刻。虽然我们研究的是连续决策问题，可是再“连续”的状态，也是由一个个小的、离散的静态状态对象构成的，Agent只能处理这类离散的对象。 ⏱ 2023-10-09 08:45:18</p></blockquote><blockquote><p>📌 我们基本上可以认为，动作的取值范围是客观的。 ⏱ 2023-10-09 08:46:36</p></blockquote><blockquote><p>📌 状态值的数量可能比较多，因为要描述一个具体的状态，几个离散的数据可能不够用，通常需要数以万计甚至数以百万计的不同数值才能描述一个环境所能产生的状态值。它的取值基本上可以认为是主观的——因为人可以根据自己的需要对客观世界进行不同维度的刻画，所以 Agent也可以根据自己的需要对它所感知的世界进行相应的取舍。 ⏱ 2023-10-09 08:47:10</p></blockquote><blockquote><p>📌 奖励值估计是这三个值中最特殊的一个了，它几乎完全由机器人的开发者来决定。机器人每观测到一个状态，都要尝试做一个动作（原地不动也算一个动作，你可以把它叫作“沉默”）。那么，一个动作应该获得多大的奖励值呢？环境本身是无法给出的。 ⏱ 2023-10-09 08:48:01</p></blockquote><blockquote><p>📌 其实，关键就在奖励值的设定上。对一个机器人来说，在一个环境中观测到一个状态值后做一个动作所得到的奖励值，直接体现了训练者的价值观，而这个价值观将对训练结果产生直接的影响。 ⏱ 2023-10-09 08:48:21</p></blockquote><h3 id="第2章-强化学习的脉络" tabindex="-1">第2章 强化学习的脉络 <a class="header-anchor" href="#第2章-强化学习的脉络" aria-label="Permalink to &quot;第2章 强化学习的脉络&quot;">​</a></h3><blockquote><p>📌 与其说第1章介绍了强化学习建模时研究的对象，不如说是不得不做的对客观世界的观察手段的片段性陈述。 ⏱ 2023-10-09 08:57:57</p></blockquote><blockquote><p>📌 我们应该如何理解机器人行为范畴内的策略呢？一般用这样一个表达式来描述：a=π(s).这里的a指的是动作，s指的是状态，函数π（跟圆周率没有半点关系）就是用来描述策略的。从形式上看，策略就像一个函数，只要输入一个状态s，就输出一个动作a。 ⏱ 2023-10-09 08:58:31</p></blockquote><blockquote><p>📌 这个θ取什么值合适，在强化学习中是通过训练得到的。θ的表达式通常不会太简单，不是用一两个实数就能描述清楚的。 ⏱ 2023-10-09 09:00:45</p></blockquote><blockquote><p>📌 人类研究的策略其实是非常复杂的 ⏱ 2023-10-09 09:05:47</p></blockquote><blockquote><p>📌 不能像人一样进行复杂的逻辑推理与判断，进行“理性+感性”的复杂分析，只能比较一个“数”的大小。什么“数”？就是我们常说的评价函数（Evaluation Function）。 ⏱ 2023-10-09 09:06:08</p></blockquote><blockquote><p>📌 对一个机器人所处的环境来说，如果这个环境是自然形成的，而不是由某个人创造的，那么，这个环境将以什么样的规律变化，通常是没办法直接表示出来的 ⏱ 2023-10-09 09:07:27</p></blockquote><blockquote><p>📌 另一种情况是，如果这个环境是由别人创造的，我们在看不到任何源代码的情况下，其实没有什么好办法，只能一次次试探，在试探中通过统计来得到转移概率。根据大数定律[插图]，只要试探的次数足够多，这个转移概率矩阵就会“稳定”下来，转移概率的值会趋近于某个数字。 ⏱ 2023-10-09 09:08:56</p></blockquote><blockquote><p>📌 。既然如此，在任何一个场景中，我们敢说自己观测到了所有的状态、将转移概率的统计收敛做到位了吗？我们很可能连状态的设置都做得不够充分，又怎么能得到一个理想的、完美的模型呢？ ⏱ 2023-10-09 09:12:21</p></blockquote><h2 id="第1章-强化学习是什么-1" tabindex="-1">第1章 强化学习是什么 <a class="header-anchor" href="#第1章-强化学习是什么-1" aria-label="Permalink to &quot;第1章 强化学习是什么&quot;">​</a></h2><h3 id="划线评论" tabindex="-1">划线评论 <a class="header-anchor" href="#划线评论" aria-label="Permalink to &quot;划线评论&quot;">​</a></h3><blockquote><p>📌 这里的θ就是x前面的系数2和后面的常数项3。<br> - 💭 特征值 - ⏱ 2023-10-08 08:46:48</p></blockquote>',49)])])}const s=e(u,[["render",r]]);export{n as __pageData,s as default};
